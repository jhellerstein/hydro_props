## DBSP is pretty simple for us to emulate
This assumes we have an Abelian group based on + (commutative, with inverses).
- _Differentiate_ in DBSP takes a cumulative source and produce a delta output. It is implemented as:
```rust
in = mod -> tee();
lastR = R -> defer_tick();
in -> [pos]dR;
lastR -> [neg]dR;
dR = difference() -> mod;
```
More ideally, `difference()` is really `reduce(sum)` on inverse values:
```rust
in = mod -> tee();
lastR = R -> defer_tick();
in -> dR;
lastR -> map(|x| inverse(x)) -> dR;
dR = union() -> reduce(sum) -> mod;
```

- _Integrate_ is essentially
```rust
mod -> reduce(sum) -> mod
```
for the group operator `sum`.

- The key rewriting axioms we want to use are in Proposition 3.2 of the [DBSP arXiv paper](https://arxiv.org/abs/2203.16684).
With these we can turn a subgraph whose edges are of type Abelian group into a fully differential stream. 

With this it's fairly natural to implement, for example, multiset or set semantics. E.g.
we should be able to do all of datalog/dedalus this way. The DBSP paper gives examples, but it's all
fairly textbook. The trickiest detail is the bilinear operators like join that get rewritten as
$R \times S = dR \times dS \cup dR \times S \cup R \times dS$. We've tested this and it works in Hydroflow today.

## Note: delta vs cumulative is orthogonal to tick vs static
We can have deltas or cumulatives within a tick or across ticks.

## What if we have edges that are not Abelian Groups?
These edges need to work with cumulatives since we cant implement `inverse`. Hence we need to support
flows that are a mix of deltas and cumulatives in the runtime.

## Current concerns
- To save work, can we passing cumulatives by reference rather than by value?
- Our current scheduling sometimes wants to reference unchanged upstream cumulatives for binary ops
  with `static` semantics implemented via `persist` ops.
  Example in current
  (non-DBSP-emulating) hydroflow is
  ```rust
   left = R -> persist() -> tee();
   right = S -> persist() -> tee();
   left -> ... <otherstuff>;
   right -> ... <moreotherstuff>;

   left -> [0]join::<'tick>;
   right -> filter(...) -> [1]join;
   ```
  The interesting bit is the last line -- when we get a new `left` tuple, we need to join against all the
  `right` tuples so we want to "replay" the `persist()` on `S` (the second line).

Questions:
1. is this even worth supporting, or does a `persist()` need to be at the input to the join in all cases?
    - If we require/inject a `persist` for any such join, would redundant `persist`s be easy to "rewrite away"
    - What are the rewriting rules for `persist` -- i.e. what does it commute with?
2. For streams that are Abelian groups, does the bilinear delta rewrite above make this problem go away?
    - Note: even if this is true, we need to solve the problem for other subgraphs that aren't delta-rewritable!
3. How would we rethink this example in terms of cumulative and delta edge types?
    - Version 1: assume this is a (differentiable) relational example (which it is, given the use of `join`)
    - Version 2: assume it's a non-differentiable, non-Abelian-group latticeflow example. Is scheduling still hard?

Version 2 (all inputs are cumulative type, assume `'tick` semantics so we don't have to think about persistence over time yet):
```rust
   cumu_left = R -> tee();
   cumu_right = S -> tee();
   cumu_left -> ... <otherstuff>;
   cumu_right -> ... <moreotherstuff>;

   cumu_left -> [0]the_join;
   cumu_right -> filter(...) -> [1]the_join;
   join = join()::<'tick>;
```
Here it's clear we won't have sharing: we can't use the cumulative from `right` in the join, since `filter` computes 
a different cumulative and (in general) we can't "peek" inside that cumulative and extract a "sublattice"
corresponding to filter. Hence `filter` will pass a different cumulative to `join` -- whether it's by value or by ref,
the point is that it is passed on a single edge without sharing.

From here, if the above *was* an Abelian group, we'd apply the DBSP chain rule to differentiate it from the outside in.
And it would look like the following:
```rust
   dleft = R -> delta() -> tee();
   dright = S -> delta() -> tee();
   dleft -> ... <otherstuff>;
   dright -> ... <moreotherstuff>;

   dleft -> [0]the_join;
   dright -> filter(...) -> [1]the_join;
   the_join = delta_join_tick;
```
where `delta_join_tick` is the macro for $dR \times dS \cup dR \times S \cup R \times dS$.

Here the only accumulation/persistence required is inside the join macro, where we materialize $R$ and $S$.

## Additional Thoughts, 12/7 evening
1. DBSP assumes you have an Abelian group, so you have an identity item $0$ and every item $s$ in the domain has an inverse such that $s + s^{-1} = 0$. Let’s assume this is a common case for us, but not universal (DBSP reviews tricks to get relational semantics using this, for example). So we should exploit DBSP tricks when we can, but we shouldn’t count on them. Said differently, we probably want a mixed algebra model, that annotates graph edges with algebraic properties, and we only exploit those that we can where we can. This might push us into interesting new territory.
2. The temporal model of DBSP aligns nicely with that of Hydroflow (inherited from Dedalus): totally-ordered time, and the ability to “compare” across 1 time offset.
    - Dedalus/Hydroflow allows us to defer updates to one tick in the future.
    - DBSP has a $z^{-1}$ operator that lets you examine the state of its input one tick in the past.
    - These are interchangeable .. in Dedalus if we want to implement $z^{-1}$ for a relation $r$ we define an IDB for it:  `z_r@t+1 :- r`  (or in Hydroflow a variable like `z_r = r -> defer_tick()`).
3. As a result of (2) the rewrites in the DBSP paper for delta management port directly to Hydroflow. @Chris Douglas and I have implementations of delta-rewritten join and distinct in a fork of the Hydroflow repo. The other trickyish example in the DBSP arxiv paper is recursion, datalog-style, which I’ll do next. I think it’ll also be direct.
    - With these examples in place, the next step is a general proof. There is a nice small proposition in the DBSP paper regarding the key rewrites that we should port to Hydroflow-over-Abelian-groups
    - With that done we should ensure that these rewrites cause no harm for parts of a Hydroflow program that aren’t over Abelian Groups.
4. In my mind there’s still a disconnect between the Hydroflow notions of persistence and DBSP’s _differentiation_ vs _integration_ (the word "delta" is used in both so I’ll try to use _differential_ when discussing DBSP). One thing I toyed with was to look at all combinations of Hydroflow’s `'static` vs `'tick` with DBSP’s _differential_ vs _integral_, but not all combinatios make sense. The definition of _differential_ in DBSP is in terms of the $z{^-1}$ operator, which “subtracts” the last tick from the current tick --- so you can’t create differentials within a single timestep.  
    - One hypothesis is DBSP only can express `'static` semantics anyhow, so we shouldn’t hope to mix DBSP reasoning with parts of a Hydroflow graph that use `'tick`. 
    - Or maybe the hypothesis is false, but it’s just awkward to get `'tick` semantics in DBSP because it is “persistent by default”, where Dedalus is “ephemeral by default”. If so, we need a DBSP design pattern for “ephemerality circuits” (complementing Dedalus’ “persistence rules”), which at the start of each tick “inverts” (i.e. deletes) all items from the previous tick.
    - Note on the above: to implement inverting "at the start of each tick" in DBSP, we might need to "insert a tick" between each pair to delete `'tick` stuff, and add "persistence rules" to DBSP to get $z^{-1}$ to work after the second tick for those collections that are `'static`.
5. Even if we can implement 'tick semantics in DBSP, I'm _still_ not sure what it could mean to do differential computation "within a tick", and that concerns me a bit as we clearly want to flow large collections along edges within a single tick.

## More on semi-naive eval and implications, 12/17 night
I've been thinking about how to write semi-naive evaluation in Hydroflow. We don't quite have everything we need for it --- we need to allow persistence at the level of a single iteration, like what's currently done by the operator `next_stratum()`. That's not supported in the rest of our ops (`join`, `fold`, etc.)Some thoughts on tackling this:

1.  **Per-iteration time granularity:**  As we've contemplated, we should probably have persistence be an explicit op of its own always, with state shared by the op downstream. Then we can have `persist::<timescale>` and `defer::<timescale>` where timescale is one of (`'iteration`, `'tick` or `'static`). Then no more temporal variants of `join`, `fold`, etc., rather we prepend them with appropriate `persist` ops.

    - Note that to make this work with `fold` and `reduce` we need to clean up the definition of `persist` so it's basically a `fold`: it should handle general "growth over time", not necessarily in the form of growing a collection. If everything is a lattice this is just `lattice_merge` but we haven't settled on whether everything is a lattice so maybe more like an arbitrary `fold`.

1.  **The utility of multiple time grains:** The datalog-inspired languages at Berkeley always had a single clock per node --- semi-naive was done under the covers in the implementation of Bloom. By contrast, differential dataflow and DBSP (and timely dataflow) allow arbitrary nesting of such timescales.

    1. I've argued in the past that the only semantic uses for a clock are to avoid cycles through non-monotonicity. This is the amount of "happens-before" needed for a sequential thread, and the remainder is communication across such clocks.
        - Classic example:  `p() :- ¬p()` is a paradox. `p()@t+1 :- ¬p` is just a toggle. The clock is needed to differentiate those.
        - In a datalog lens, Dedalus ensures what's called local stratification via the use of time: "local stratification" means that every deduced fact has a finite acyclic proof tree, which is guaranteed by the total order of time and the rules of Dedalus requiring stratification within a tick.

    1.  It's clear from the semi-naive example that a second level of nesting can help with performance, specifically by enabling incremental maintenance (delta handling) in cyclic flows.
    1.  Following from Timely and Differential Dataflow, DBSP embraces abitrary clock depth. But the examples in the arXiv paper are based on Datalog so very much in our world of 2 levels:

        - In the DBSP examples, nested "iterations" are used for fixpoint --- `while (deltas are derived)`. Outermost "ticks" in DBSP correspond to `while (deltas arrive from "the outside")`.

        - They don't need to discuss non-monotonicity because they assume inverses of all values (Abelian Groups), and hence "difference" is a kind of "sum".

    1.  **Open theory question**: Can >2 clocks be beneficial in space or time performance? In what way? If I give you a *tree* of `while` expressions rather than just a simple nested pair, what execution tricks could you play for that tree and what benefit could you get? This would be nice to answer. .

1.  **Hydroflow design questions:** The DBSP paper achieves arbitrarily nested clocks and rewritings rather elegantly: basically their streams are themselves Abelian groups so streams of streams are as well, and you can differentiate (use a clock to iterate) at each nesting.

    - **Rethink in light of DBSP:** Can and should we reframe Hydroflow's persistence akin to DBSP, even if we only have 2 levels of time?

        - **Can we:** in DBSP, deltas are defined wrt the inverse of the Abelian Group ($current\_tick + past\_tick^{-1}$). Can we define deltas in a general way? Alternatively, in Conor's terms, can we always "upgrade" local streams to Abelian Groups, and live in DBSP's semantics?
        - **Should we:** The two "clocks" in the hydroflow semantics are reflected in our runtime. Would the runtime get simpler or more complicated if we allow DBSP's arbitrary clock nesting? What about the dataflow operators? Would make sense to look at the DBSP implementation.

1.  **Lattice deltas meet Incremental computation:** In Hydroflow we originally posited that a dataflow edge could be passing a lattice point as a single object, or as a stream of sub-objects (that we were calling *deltas*), and we should type our edges accordingly so the operators are well-behaved. How does this relate to the incremental computation notion of differentiation in the DBSP world?

    1.  On a fussy level, there are the algebraic differences between lattices and Abelian groups, but I'm not sure that matters.
    2.  They don't discuss in the paper how they track the types of their edges, though there's an implicit type system in there. Maybe that's all we'r discussing --- explicit typing of what's a delta edge (iterated) and what's a cumulated edge (not iterated).

        - Arguably the real question we've been stuck on when and how we get delta edges vs cumulative edges. DBSP answers that in a walled garden of Abelian groups. I wonder if things get harder for us if we have scenarios over lattices rather than groups.
